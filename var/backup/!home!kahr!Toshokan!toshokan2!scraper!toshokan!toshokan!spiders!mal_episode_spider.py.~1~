import scrapy

class MALEpisodeSpider(scrapy.Spider):
    """Scraper for episode pages on MyAnimeList."""
    name = "MAL_episodes"

    def start_requests(self):
        for anime_id in range(1, 40000):
            yield scrapy.Request(url="https://myanimelist.net/anime/" + str(anime_id), callback=self.parse)

    def parse(self, response):
        # URL: of the format "https://myanimelist.net/anime/1234"
        mal_id = int(response.url.split('/')[-1])

        # Title: of the format "\n<title> - MyAnimeList.net\n"
        title = response.xpath('//title/text()').extract_first().split(' - ')[0][1:]

        # General left column info; some of these still need processing later!
        try:
            episodes = int(self.get_lc(response, 'Episodes')[0])
        except:
            episodes = None

        type_:str = self.get_lc(response, 'Type')[0]
        status: str = self.get_lc(response, 'Status')[-1] # All but the last come from the "edit status" box
        aired: str = self.get_lc(response, 'Aired')[0]
        source: str = self.get_lc(response, 'Source')[0]
        duration: str = self.get_lc(response, 'Duration')[0]
        rating: str = self.get_lc(response, 'Rating')[0]

        producers: List[str] = self.get_lc(response, 'Producers')
        licensors: List[str] = self.get_lc(response, 'Licensors')
        studios: List[str] = self.get_lc(response, 'Studios')
        genres: List[str] = self.get_lc(response, 'Genres')

        description = ''.join(response.xpath(
            "//*[@id='content']/table/tr/td[2]/div[1]/table/tr[1]/td/span/text()"
        ).extract())

        raw_rels = response.xpath(
            "//*[@id='content']/table/tr/td[2]/div[1]/table/tr[2]/td/table/tr"
        ).extract()

        rels = {}

        for rel in raw_rels:
            out = []
            rel_name = rel.split('borderClass">')[1].split(':')[0]
            rel_links = [x.replace(' ', '') for x in rel.split('borderClass">')[2].split('>,')]
            for link in rel_links:
                split_link = link.split('/')
                # Links might be borked and empty
                try:
                    out.append((split_link[1], int(split_link[2])))
                except:
                    pass

            rels[rel_name] = out

        ops = [x.replace(': ', '') for x in response.xpath(
            "//*[@id='content']/table/tr/td[2]/div[1]/table/tr[2]/td/div[3]/div[1]/div/span/text()"
        ).extract()]

        eds = [x.replace(': ', '') for x in response.xpath(
            "//*[@id='content']/table/tr/td[2]/div[1]/table/tr[2]/td/div[3]/div[3]/div/span/text()"
        ).extract()]

        main_img = response.xpath(
            "//*[@id='content']/table/tr/td[1]/div/div[1]/a/img/@src"
        ).extract_first()

        synonyms = []
        for x in response.xpath("//div[@class='spaceit_pad']").extract()[:3]:
            try:
                synonyms.append(x.split('">')[2].replace('\n  </div>','').replace('</span> ',''))
            except:
                pass

        yield {
            'mal_id': mal_id,
            'title': title,
            'num_episodes': episodes,
            'status': status,
            'type': type_,
            'aired': aired,
            'source': source,
            'duration': duration,
            'rating': rating,
            'producers': producers,
            'licensors': licensors,
            'studios': studios,
            'genres': genres,
            'description': description,
            'relations': rels,
            'OPs': ops,
            'EDs': eds,
            'main_img': main_img,
            'synonyms': synonyms
        }

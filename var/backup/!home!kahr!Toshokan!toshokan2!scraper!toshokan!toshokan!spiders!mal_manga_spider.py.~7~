import scrapy
from typing import List
import logging

class MALMangaSpider(scrapy.Spider):
    """Scraper for manga pages on MyAnimeList."""
    name = "MAL_manga"
    _lc_xpath: str = "//div[@class='js-scrollfix-bottom']/div[contains(.//span, '{}:')]/descendant-or-self::*/text()"
    def __init__(self,  *args, **kwargs):
        super(MALMangaSpider, self).__init__(*args, **kwargs)
        logging.getLogger('scrapy').setLevel(logging.WARNING)

    def get_lc(self, response, text) -> List[str]:
        raw: List(str) = response.xpath(self._lc_xpath.format(text)).extract()
        results = []
        for r in raw:
            r = r.replace('  ',' ').replace('\n','').strip()
            if r and r != ' ' and r != text + ':' and r != ',':
                results.append(r)
        return results

    def start_requests(self):
<<<<<<< HEAD
        for manga_id in range(1, 150000):
            yield scrapy.Request(url="https://myanimelist.net/manga/" + str(manga_id), callback=self.parse)
=======
        for manga_id in range(1, 10):
            yield scrapy.Request(url=self.settings["MAL_MANGA_PATH"] % str(manga_id), callback=self.parse)
>>>>>>> e5b6f478d85e08c5a4d12fe1fab3faa196ba74d1

    def parse(self, response):
        # URL: of the format "file:///manga/ID.html"
        mal_id = int(response.url.split('/')[-1].split(".")[0])

        is404 = response.xpath('//*[@id="contentWrapper"]/div[1]/h1/text()').extract_first()
        is404option2 = response.xpath('/html/head/title/text()').extract_first()
        if is404 == "404 Not Found" or is404option2 == "404 Not Found":
            print("%s %s"%(str(mal_id),(is404 or is404option2)))
            yield None
            return

        print("%s %s"%(str(mal_id), "Found."))

        title = response.xpath('//span[@itemprop="name"]/text()').extract_first()

        try:
            num_volumes = int(self.get_lc(response, 'Volumes')[0])
        except:
            num_volumes = None

        try:
            num_chapters = int(self.get_lc(response, 'Chapters')[0])
        except:
            num_chapters = None

        type_:str = self.get_lc(response, 'Type')[0]
        status: str = self.get_lc(response, 'Status')[-1]
        published: str = self.get_lc(response, 'Published')[0]
        serialization: List[str] = self.get_lc(response, 'Serialization')
        genres: List[str] = self.get_lc(response, 'Genres')

        authors = []
        for a in response.xpath("//div[descendant::span[@class='dark_text' and text() = 'Authors:']]").extract()[-1].split('<a')[1:]:
            authors.append({
                'name': a.split('">')[1].split('</a>')[0],
                'id': int(a.split('/people/')[1].split('/')[0]),
                'roles': a.split('(')[1].split(')')[0].split(' &amp; ')
            })

        description = ''.join(response.xpath(
            "//*[@id='content']/table/tr/td[2]/div[1]/table/tr[1]/td/span"
        ).extract())

        raw_rels = response.xpath(
            "//table[@class='anime_detail_related_anime']//tr"
        ).extract()

        rels = {}
        for rel in raw_rels:
            out = []
            rel_name = rel.split('borderClass">')[1].split(':')[0]
            rel_links = [x.replace(' ', '') for x in rel.split('borderClass">')[2].split('>,')]
            for link in rel_links:
                split_link = link.split('/')
                # Links might be borked and empty
                try:
                    out.append((split_link[1], int(split_link[2])))
                except:
                    pass

            rels[rel_name] = out

        main_img = response.xpath(
            "//*[@id='content']/table/tr/td[1]/div/div[1]/a/img/@src"
        ).extract_first()

        synonyms = []
        for x in response.xpath("//div[@class='spaceit_pad']").extract()[:3]:
            try:
                synonyms.append(x.split('">')[2].replace('\n  </div>','').replace('</span> ',''))
            except:
                pass

        yield {
            'mal_id':mal_id,
            'title':title,
            'num_volumes':num_volumes,
            'num_chapters':num_chapters,
            'type':type_,
            'status':status,
            'published':published,
            'authors':authors,
            'serialization':serialization,
            'genres':genres,
            'description':description,
            'relations':rels,
            'main_img':main_img,
            'synonyms':synonyms
        }

import scrapy
import os
from typing import List

class MALAnimeSpider(scrapy.Spider):
    """Scraper for anime pages on MyAnimeList."""
    name: str = "MAL_anime"
    # xpath for getting things in the left column
    _lc_xpath: str = "//div[@class='js-scrollfix-bottom']/div[contains(., '{}')]/descendant-or-self::*/text()"

    def get_lc(self, response, text) -> List[str]:
        """Gets data from the column on the left of the page.

        Args:
            response (`scrapy.http.HtmlResponse`_): The scrapy response.
            text (str): The text to search for.
        
        Returns:
            List[str]: results
        
        
        Example:
            ::

                >>> get_lc(response, "Studios")
                ['Gainax', 'Production I.G']

        Todo:
            * We might want to retain information such as hyperlinks; that is discarded right now.
        
        .. _scrapy.http.HtmlResponse:
           https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.HtmlResponse
        """
        raw: List(str) = response.xpath(self._lc_xpath.format(text)).extract()
        results = []
        for r in raw:
            r = r.replace('  ',' ').replace('\n','').strip()
            if r and r != ' ' and r != text + ':' and r != ',':
                results.append(r)
        return results

    def start_requests(self):
        def new_cookie():
            # fite me irl
            return os.popen("curl -I 'https://myanimelist.net/' -H 'authority: myanimelist.net' -H 'pragma: no-cache' -H 'cache-control: no-cache' -H 'upgrade-insecure-requests: 1' -H 'user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36' -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8' -H 'accept-encoding: gzip, deflate, br' -H 'accept-language: en,it-IT;q=0.9,it;q=0.8,en-US;q=0.7,de;q=0.6,fr;q=0.5,zh-CN;q=0.4,zh;q=0.3' --compressed | grep MALSESSIONID | cut -d '=' -f2 | cut -d ';' -f1").read().replace('\n', '')

        cookie = new_cookie()
        for anime_id in range(1, 40000):
            if anime_id % 5000 == 0:
                cookie = new_cookie()

            yield scrapy.Request(url="https://myanimelist.net/anime/" + anime_id, callback=self.parse, cookies={'MALSESSIONID': cookie})

    def parse(self, response):
        # URL: of the format "https://myanimelist.net/anime/1234"
        mal_id = int(response.url.split('/')[-1])

        # Title: of the format "\n<title> - MyAnimeList.net\n"
        title = response.xpath('//title/text()').extract_first().split(' - ')[0][1:]

        # General left column info; some of these still need processing later!
        episodes: int = int(self.get_lc(response, 'Episodes')[0])

        type_:str = self.get_lc(response, 'Type')[0]
        status: str = self.get_lc(response, 'Status')[-1] # All but the last come from the "edit status" box
        aired: str = self.get_lc(response, 'Aired')[0]
        source: str = self.get_lc(response, 'Source')[0]
        duration: str = self.get_lc(response, 'Duration')[0]
        rating: str = self.get_lc(response, 'Rating')[0]

        producers: List[str] = self.get_lc(response, 'Producers')
        licensors: List[str] = self.get_lc(response, 'Licensors')
        studios: List[str] = self.get_lc(response, 'Studios')
        genres: List[str] = self.get_lc(response, 'Genres')
